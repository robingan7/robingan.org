<table name="project" id="Google_Science_Fair_2018">
        <date>2018.11-2018.12</date>
        <column name="title">Google Science Fair 2018</column>
        <link>
            <![CDATA[
            <li>
                <a>Link</a>
                <ul>
                    <li>
                        <a href="https://www.googlesciencefair.com/projects/2018/400b40c695bbac6ee5c2a2efb5313c6126f3469797bbdc7ba4f2a56d78dd34cb">Project</a>
                    </li>
                    <li>
                        <a href="https://github.com/robingan7/TrashSort">View Code</a>
                    </li>
                </ul>
            </li>
            <li>
                <a>Team</a>
                <ul>
                    <li>
                        <a href="http://alexlynd.com">Alex Lynd</a>
                    </li>

                    <li>
                        <a href="https://www.instagram.com/car_in_a__">Carina Bachmann</a>
                    </li>

                    <li>
                        <a href="https://robingan.org">Robin Gan</a>
                    </li>
                </ul>
            </li>
            <li>
                <a>Gallery</a>
                <ul>
                    <li>
                        <a href="https://www.youtube.com/watch?v=fyWvKE-A49Qlist=PLQRNcG6B1Zs8ZZm5aC7aDdEiJAIN3Tt9I">Video</a>
                    </li>
                </ul>
            </li>
            ]]>
        </link>
        <content>
            <![CDATA[
            <h3>Overview</h3>
            <p>I, Robin, was born in China, where issues with trash classification are evident and became more apparent when I moved to America last year. My passion for computer science led me to solve this problem by using AI <a href="https://www.tensorflow.org/">tensorflow</a> convolutional neural network (written in Python) graph by <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">TensorBoard</a> which is perfect for image recognition. My friend <a href="http://alexlynd.com/">Alex</a> used electronics (node MUC, lamp, raspberry pi) to capture images of trash to be executed in the program for sorting types of trash. Carina helped revise the report.</p>
            <h3>About the problem</h3>
            <p>
                This problem is very conspicuous in our society because people are not aware of proper trash sorting. For example, I surveyed my classmates and sorted through the trash cans at my school and home. Here are some pictures
                As you can see, many recyclables were placed in the trash bin designated for landfill.If the majority of the population’s trashcans resemble this, the problem is sure to multiply. In fact, about 28 billion recyclable bottles and jars end up in American landfills every year. This research shocked and encouraged me to solve this problem as it showed how prevalent and serious this is. My hypothesis is that my solution will classify 95% of the refuse and recyclables.
            </p>
            <blockquote>
            <p>Bangalore’s garbage troubles stem directly from its booming high-tech economy. The city’s population of 10 million has grown by 50 percent since 2001, and at least 3 million more Indians are expected to migrate to Bangalore in the next decade. - <a href="https://www.theatlantic.com/international/archive/2014/06/confessions-of-a-trash-tourist-india/373118/">NOAH M. SACHS</a></p>
            </blockquote>
            <p>
                <img src="/images/project/Google_Science_Fair_2018/plastic_in_trash.jpeg" alt="plastic_in_trash">
            </p>
            <h3>Research</h3>
            <h4>Trash / Sorting</h4>
            <p>We decided to look at how our local community and school tends to trash sorting for our research. First, we conducted this survey which clearly indicated that people are not taking the responsibility of trash sorting upon themselves, and it is evident of a problem in our community. Additionally, video below shows Robin Gan sorting some trash cans from our school, which expresses that students are not recycling, even with two adjacent distinguishable bins in plain sight.  The data we collected from our survey and the trash dive concludes that the task of sorting refuse is not prioritized.</p>
            
            <h4>AI / Project Technology</h4>
            <p>This system needs a platform that can support heavy proccessing, web hosting, image capturing, and digital IO.  The <a href="https://www.raspberrypi.org/magpi/pi-zero-w/">Raspberry Pi 0W (computer)</a> can handle all of this at the cost of $10.  Additionally, we will use Python language for coding since it has the most support and a lot of libraries/documentation.  We then delved 
                into further research on the operation of Al and implementation. 
            </p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/8aGocIjT1l0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            
            <h3>Code & Algorithm Explanation</h3>
            <h4>Code Structure</h4>
            <ul>
                <li>Load Images(pickle)</li>
                <li>Add Layers</li>
                <li>Process through activiation & regulation functions</li>
                <li>Compile & Save trained model</li>
            </ul>
            
            <p><strong>First layer</strong>, using a 64-neurons convolutional layer with ReLU activation function. </p>
            <figure class="highlight"><pre><code class="language-js" data-lang="js">model = Sequential()
model.add(Conv2D(64,(3,3), 
          input_shape = X.shape[1:]))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))</code>
            </pre>
            </figure>
            <hr/>

             <p><strong>Second layer</strong>, implemented the same layers structure. </p>
            <figure class="highlight"><pre><code class="language-js" data-lang="js">model.add(Conv2D(64,(3,3)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))</code>
            </pre>
            </figure>
            <hr/>
            <p>
                <strong>Thrid layer</strong>
                , flattenng to a 256-neurons dense layer, also activated with relu function. Moreover, the layer has a dropout of 0.2
                to avoid overfitting
            </p>
            <figure class="highlight"><pre><code class="language-js" data-lang="js">model.add(Flatten())
model.add(Dense(256,activation='relu',
          kernel_regularizer=regularizers.l2(0.001)))
model.add(Dropout(0.2))</code>
            </pre>
            </figure>
            <hr />
            <h4>Blogs</h4>
            <p> I tested 23 combinations of dropout, neurons per layers and number of input images in order to get a 
                higher validation accuracy. Here are some notes I took when I expereinced different combinations.</p>
            <ul>
                <li>
                    <p><strong>Model 1:</strong> This is the first experiment. It is with 0.3 dropout, 64 neurons per layer, no regularization and 7000 input images.</p>
                    <img src="/images/project/Google_Science_Fair_2018/11.png" alt="plastic_in_trash">
                    <img src="/images/project/Google_Science_Fair_2018/12.png" alt="plastic_in_trash">
                    <p>From the graph, the testing accuracy(batch_acc) ended up with 98% and continued growing during the training. However, the validation accuracy(epoch_val_acc) floats between 83% and 85%. When I tested this program with images from real life, it made lots of mistakes due to its low validation accuracy. </p>
                </li>

                <li>
                    <p>
                        <strong>Model 8:</strong>
                        This program is with 0.4 dropout, 64 neurons per layer, 7000 input images, and 0.0001 regularization.
                    </p>
                    <img src="/images/project/Google_Science_Fair_2018/81.png" alt="plastic_in_trash">
                    <img src="/images/project/Google_Science_Fair_2018/82.png" alt="plastic_in_trash">
                    <p>Although the accuracies are both lower than the first. But we can see the greater dropout and regularization. The validation accuracy is raising while testing accuracy is floating near 90%. From the behavior of testing accuracy, I conclude that the program has the underfitting problem. To solve this I lowered the dropout regularization and increased the number of neurons to balance the loss. I did not, however, change all these in one step.  </p>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/cfVT8qjilFg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </li>

                <li>
                    <p>
                        <strong>Model 12:</strong>
                        This program is with 0.3 dropout, 0.001 regularization, 7000 input images and 128 neurons per layer.
                    </p>
                    <img src="/images/project/Google_Science_Fair_2018/121.png" alt="plastic_in_trash">
                    <img src="/images/project/Google_Science_Fair_2018/122.png" alt="plastic_in_trash">
                    <p>During this time I really struggled. Both testing accuracy and validation accuracy stayed the same. I actually stopped in the middle of the training because the values were almost the same. After some other research, I realize the high dropout might be the reason it can not be accurate. So in the next experiments, I lowered the dropout and the neurons per layer.  </p>
                </li>

                <li>
                    <p><strong>Model 21:</strong> This is the first experiment. It is with 0.3 dropout, 64 neurons per layer, no regularization and 7000 input images.</p>
                    <img src="/images/project/Google_Science_Fair_2018/192.png" alt="plastic_in_trash">
                    <img src="/images/project/Google_Science_Fair_2018/221.png" alt="plastic_in_trash">
                    <p>From the graph, the testing accuracy(batch_acc) ended up with 98% and continued growing during the training. However, the validation accuracy(epoch_val_acc) floats between 83% and 85%. When I tested this program with images from real life, it made lots of mistakes due to its low validation accuracy. </p>
                </li>

                <li>
                    <p>
                        <strong>Model 23:</strong>
                        This one is the exact the same one as last program, except that there are 30,000 input images.
                    </p>
                    <img src="/images/project/Google_Science_Fair_2018/231.png" alt="plastic_in_trash">
                    <img src="/images/project/Google_Science_Fair_2018/232.png" alt="plastic_in_trash">
                    <p>This one is perfect and ready to go. Both accuracies are above 96%. Then, I try another one with 20 epoch. It has impressive 98% accuracies. Here is the application of this model.  </p>
                </li>
            </ul>  
            
            <h3>Conclusion</h3>
            <p>I have a very successful training model with 98% accuracy as shown in the youtube video. It can distinguish most of the trash in our life. When I went to a dining hall, canteen or restaurant, I always take some pictures of the trash. And I will upload to program, it got them correct every time. Here is the video of testing the program.</p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/fyWvKE-A49Q" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <p>You might think about how this can be used in our real life. Alex uses image capturing, digital IO, and Raspberry Pi 0W (computer) to implement the AI program as an actual product. Here are a video and a sketch about the electronics design. </p>
            <img src="/images/project/Google_Science_Fair_2018/electronics.jpg" alt="plastic_in_trash"> 
                
            ]]>
        </content>
    </table>